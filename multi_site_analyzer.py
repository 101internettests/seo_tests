#!/usr/bin/env python3
"""
–ú—É–ª—å—Ç–∏—Å–∞–π—Ç–æ–≤—ã–π SEO –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä –¥–ª—è –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤
"""

import os
import sys
import argparse
import logging
import json
import requests
import time
from datetime import datetime
from typing import List, Dict, Any
from bs4 import BeautifulSoup

from google_sheets_service_account import GoogleSheetsServiceAccount
from telegram_bot import TelegramBot

# –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ –±—É–¥–µ—Ç –Ω–∞—Å—Ç—Ä–æ–µ–Ω–æ –≤ main()

logger = logging.getLogger(__name__)


# –ü–æ–¥—Å–∫–∞–∑–∫–∏ –¥–ª—è —Ä–∞—Å–ø–æ–∑–Ω–∞–≤–∞–Ω–∏—è —Å—Ç—Ä–∞–Ω–∏—Ü –∑–∞—â–∏—Ç—ã –∏ —Ç–∏–ø–∏—á–Ω—ã—Ö —Å—Ç—Ä–∞–Ω–∏—Ü –æ—à–∏–±–æ–∫
_PROTECTION_HINTS = [
    'just a moment',
    'ddos protection',
    'cloudflare',
    'captcha',
    'please enable cookies'
]

_ERROR_PAGE_HINTS = [
    '404',
    'not found',
    '–æ—à–∏–±–∫–∞',
    'server error',
    'page not found',
    'access denied',
    'forbidden',
    'bad gateway',
    'service unavailable'
]


class SEOParser:
    """–ü—Ä–æ—Å—Ç–æ–π SEO –ø–∞—Ä—Å–µ—Ä –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤"""
    
    def __init__(self, delay_between_requests: float = 2.0, config: Dict[str, Any] = None, sheets_manager=None, max_retries: int = 2, backoff_seconds: float = 0.7, ignore_protection: bool = False, use_cloudscraper: bool = False):
        self.delay_between_requests = delay_between_requests
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º HTTP-—Å–µ—Å—Å–∏—é
        if use_cloudscraper:
            try:
                import cloudscraper  # type: ignore
                self.session = cloudscraper.create_scraper()
                logger.info("–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω cloudscraper –¥–ª—è HTTP-—Å–µ—Å—Å–∏–∏")
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å cloudscraper, –∏—Å–ø–æ–ª—å–∑—É–µ–º requests.Session(): {e}")
                self.session = requests.Session()
        else:
            self.session = requests.Session()
        self.session.headers.update({
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36'
        })
        self.config = config
        self.sheets_manager = sheets_manager
        self.max_retries = max_retries
        self.backoff_seconds = backoff_seconds
        self.ignore_protection = ignore_protection
    
    def analyze_page(self, url: str) -> Dict[str, Any]:
        """
        –ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã
        
        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
            
        Returns:
            –†–µ–∑—É–ª—å—Ç–∞—Ç –∞–Ω–∞–ª–∏–∑–∞
        """
        result = {
            'url': url,
            'timestamp': datetime.now().isoformat(),
            'status': 'error',
            'error': None,
            'headings': {},
            'comparison': {}
        }
        
        try:
            last_error = None

            for attempt in range(self.max_retries + 1):
                try:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º requests.get, —á—Ç–æ–±—ã —É–¥–æ–±–Ω–µ–µ –±—ã–ª–æ –º–æ–∫–∞—Ç—å –≤ —Ç–µ—Å—Ç–∞—Ö
                    response = self.session.get(
                        url,
                        headers=self.session.headers,
                        timeout=30
                    )

                    # –ü–æ–≤—Ç–æ—Ä—è–µ–º –ø—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö —Å–µ—Ä–≤–µ—Ä–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö
                    if 500 <= response.status_code < 600:
                        raise requests.exceptions.HTTPError(f"Server error {response.status_code}")

                    text_lower = (response.text or '').lower()
                    if (not self.ignore_protection) and any(hint in text_lower for hint in _PROTECTION_HINTS):
                        raise RuntimeError("Temporary protection or captcha page detected")

                    # –ü–∞—Ä—Å–∏–º HTML
                    soup = BeautifulSoup(response.content, 'html.parser')

                    # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏
                    headings = self._analyze_headings(soup)

                    result.update({
                        'status': 'success',
                        'status_code': response.status_code,
                        'headings': headings,
                        'comparison': self._compare_with_previous(url, headings)
                    })

                    # –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏ –ø–æ—Å–ª–µ —É—Å–ø–µ—à–Ω–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞
                    if self.delay_between_requests > 0:
                        time.sleep(self.delay_between_requests)

                    last_error = None
                    break

                except (requests.exceptions.Timeout, requests.exceptions.ConnectionError, requests.exceptions.HTTPError, RuntimeError) as e:
                    last_error = e
                    if attempt < self.max_retries:
                        time.sleep(self.backoff_seconds * (2 ** attempt))
                        continue
                    else:
                        raise

        except Exception as e:
            result['error'] = str(e)
            logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {url}: {e}")
        
        return result

    def _analyze_headings(self, soup: BeautifulSoup) -> Dict[str, Any]:
        """–ê–Ω–∞–ª–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤, title –∏ description –Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–µ"""
        headings = {}

        # –ê–Ω–∞–ª–∏–∑ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤ H1-H6
        for i in range(1, 7):
            tag = f'h{i}'
            elements = soup.find_all(tag)

            total_count = len(elements)
            non_empty_count = len([el for el in elements if el.get_text(strip=True)])

            headings[f'{tag}_total'] = total_count
            headings[f'{tag}_non_empty'] = non_empty_count

        # –û–±—â–µ–µ –∫–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤
        headings['total_headings'] = sum(headings[f'h{i}_non_empty'] for i in range(1, 7))

        # –ü–æ–¥—Å—á–µ—Ç title —Ç–µ–≥–æ–≤ —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º (–∏—Å–∫–ª—é—á–∞—è —Ç–∏–ø–∏—á–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ—à–∏–±–æ–∫, –Ω–æ –Ω–µ –ª—é–±–æ–µ —Å–ª–æ–≤–æ "error")
        title_tags = soup.find_all('title')
        title_count = 0
        for title in title_tags:
            title_text = (title.get_text(strip=True) or '')
            title_text_lower = title_text.lower()
            is_typical_error = any(hint in title_text_lower for hint in _ERROR_PAGE_HINTS)
            if title_text and not is_typical_error:
                title_count += 1
        
        headings['title_count'] = title_count
        headings['title_result'] = f"Title with content: {title_count}"

        # –ü–æ–¥—Å—á–µ—Ç meta description —Ç–µ–≥–æ–≤ —Å –∫–æ–Ω—Ç–µ–Ω—Ç–æ–º (–∏—Å–∫–ª—é—á–∞—è —Ç–∏–ø–∏—á–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã –æ—à–∏–±–æ–∫)
        meta_descriptions = soup.find_all('meta', attrs={'name': 'description'})
        description_count = 0
        for meta in meta_descriptions:
            content = (meta.get('content', '') or '').strip()
            content_lower = content.lower()
            is_typical_error = any(hint in content_lower for hint in _ERROR_PAGE_HINTS)
            if content and not is_typical_error:
                description_count += 1
        
        headings['description_count'] = description_count
        headings['description_result'] = f"Description with content: {description_count}"

        return headings
    
    def _compare_with_previous(self, url: str, current_headings: Dict[str, int]) -> Dict[str, Any]:
        """–°—Ä–∞–≤–Ω–µ–Ω–∏–µ —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –∏–∑ Google Sheets"""
        if not self.config:
            return {
                'status': 'no_config',
                'changes': {},
                'errors': ['–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è –Ω–µ –ø–µ—Ä–µ–¥–∞–Ω–∞ –≤ –ø–∞—Ä—Å–µ—Ä']
            }
            
        try:
            # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ –∏–∑ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏
            settings = self.config['default_settings']
            spreadsheet_id = settings.get('spreadsheet_id')
            sheet_name = settings.get('sheet_name', '–õ–∏—Å—Ç1')
            
            if not spreadsheet_id:
                return {
                    'status': 'no_spreadsheet_id',
                    'changes': {},
                    'errors': ['–ù–µ —É–∫–∞–∑–∞–Ω ID —Ç–∞–±–ª–∏—Ü—ã –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏']
                }
            
            # –ò—Å–ø–æ–ª—å–∑—É–µ–º –ø–µ—Ä–µ–¥–∞–Ω–Ω—ã–π sheets_manager
            if not self.sheets_manager:
                return {
                    'status': 'no_sheets_manager',
                    'changes': {},
                    'errors': ['GoogleSheetsServiceAccount –Ω–µ –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω']
                }
            
            # –ü–æ–ª—É—á–∞–µ–º –≤—Å–µ –¥–∞–Ω–Ω—ã–µ –∏–∑ —Ç–∞–±–ª–∏—Ü—ã
            range_name = f'{sheet_name}!A:T'
            try:
                existing_data = self.sheets_manager.get_sheet_data(spreadsheet_id, range_name)
            except Exception as e:
                logger.warning(f"–ù–µ —É–¥–∞–ª–æ—Å—å –ø–æ–ª—É—á–∏—Ç—å –¥–∞–Ω–Ω—ã–µ –∏–∑ Google Sheets: {e}")
                return {
                    'status': 'sheets_error',
                    'changes': {},
                    'errors': [f'–û—à–∏–±–∫–∞ —á—Ç–µ–Ω–∏—è –∏–∑ Google Sheets: {e}']
                }
            
            if not existing_data or len(existing_data) <= 1:  # –¢–æ–ª—å–∫–æ –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∏–ª–∏ –ø—É—Å—Ç–æ
                return {
                    'status': 'no_previous_data',
                    'changes': {},
                    'errors': []
                }
            
            # –°—Ç—Ä–æ–∏–º –∫–∞—Ä—Ç—É –Ω–∞–∑–≤–∞–Ω–∏–π —Å—Ç–æ–ª–±—Ü–æ–≤ -> –∏–Ω–¥–µ–∫—Å
            headers = existing_data[0]
            header_to_index = {str(h).strip().lower(): idx for idx, h in enumerate(headers)}

            def find_idx(candidates):
                for name in candidates:
                    key = str(name).strip().lower()
                    if key in header_to_index:
                        return header_to_index[key]
                return None

            url_idx = find_idx(['url'])

            # –ò—â–µ–º –ø–æ—Å–ª–µ–¥–Ω–∏–µ –¥–∞–Ω–Ω—ã–µ –¥–ª—è —ç—Ç–æ–≥–æ URL
            previous_data = None
            if url_idx is not None:
                for row in reversed(existing_data[1:]):  # –ü—Ä–æ–ø—É—Å–∫–∞–µ–º –∑–∞–≥–æ–ª–æ–≤–∫–∏, –∏–¥–µ–º —Å –∫–æ–Ω—Ü–∞
                    if len(row) > url_idx and row[url_idx] == url:
                        previous_data = row
                        break
            else:
                # Fallback: –ø—Ä–µ–∂–Ω—è—è –ª–æ–≥–∏–∫–∞ —Å–æ –≤—Ç–æ—Ä—ã–º —Å—Ç–æ–ª–±—Ü–æ–º
                for row in reversed(existing_data[1:]):
                    if len(row) >= 2 and row[1] == url:
                        previous_data = row
                        break
            
            if not previous_data:
                logger.info(f"–ü—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö –¥–ª—è {url} –Ω–µ –Ω–∞–π–¥–µ–Ω–æ - —ç—Ç–æ –ø–µ—Ä–≤–∞—è –ø—Ä–æ–≤–µ—Ä–∫–∞")
                return {
                    'status': 'no_previous_data',
                    'changes': {},
                    'errors': []
                }

            # –ò–∑–≤–ª–µ–∫–∞–µ–º –¥–∞–Ω–Ω—ã–µ –æ –∑–∞–≥–æ–ª–æ–≤–∫–∞—Ö –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
            # –ü—Ä–µ–¥–ø–æ—á–∏—Ç–∞–µ–º –ø–æ–∏—Å–∫ –ø–æ –Ω–∞–∑–≤–∞–Ω–∏—è–º –∫–æ–ª–æ–Ω–æ–∫; –ø—Ä–∏ –æ—Ç—Å—É—Ç—Å—Ç–≤–∏–∏ ‚Äî –º—è–≥–∫–∏–π fallback –Ω–∞ –∏–Ω–¥–µ–∫—Å—ã
            try:
                changes = {}
                # –ù–µ–ø—É—Å—Ç—ã–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                for i in range(1, 7):
                    non_empty_idx = find_idx([f'H{i} (–Ω–µ–ø—É—Å—Ç—ã–µ)', f'h{i}_–Ω–µ–ø—É—Å—Ç—ã–µ', f'h{i}_non_empty'])
                    if non_empty_idx is not None and len(previous_data) > non_empty_idx:
                        try:
                            prev_value = int(str(previous_data[non_empty_idx]).strip())
                        except (ValueError, TypeError):
                            prev_value = 0
                    else:
                        # fallback –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º: 3..8 (0-–±–∞–∑)
                        idx = 2 + i
                        try:
                            prev_value = int(previous_data[idx]) if len(previous_data) > idx else 0
                        except (ValueError, TypeError):
                            prev_value = 0

                    current_value = current_headings.get(f'h{i}_non_empty', 0)
                    diff = current_value - prev_value
                    if diff != 0:
                        changes[f'h{i}_non_empty'] = {
                            'difference': diff,
                            'previous': prev_value,
                            'current': current_value
                        }

                # –û–±—â–∏–µ –∑–∞–≥–æ–ª–æ–≤–∫–∏
                for i in range(1, 7):
                    total_idx = find_idx([f'H{i} (–≤—Å–µ–≥–æ)', f'h{i}_–≤—Å–µ–≥–æ', f'h{i}_total'])
                    if total_idx is not None and len(previous_data) > total_idx:
                        try:
                            prev_value = int(str(previous_data[total_idx]).strip())
                        except (ValueError, TypeError):
                            prev_value = 0
                    else:
                        # fallback –ø–æ –∏–Ω–¥–µ–∫—Å–∞–º: 10..15 (0-–±–∞–∑) ‚Äî —Å–æ–æ—Ç–≤–µ—Ç—Å—Ç–≤—É—é—Ç h1..h6 total
                        idx = 8 + i
                        try:
                            prev_value = int(previous_data[idx]) if len(previous_data) > idx else 0
                        except (ValueError, TypeError):
                            prev_value = 0

                    current_value = current_headings.get(f'h{i}_total', 0)
                    diff = current_value - prev_value
                    if diff != 0:
                        changes[f'h{i}_total'] = {
                            'difference': diff,
                            'previous': prev_value,
                            'current': current_value
                        }

                # Title count
                title_idx = find_idx(['title count', 'title_count'])
                prev_title_count = 0
                if title_idx is not None and len(previous_data) > title_idx:
                    try:
                        raw = previous_data[title_idx]
                        if raw and str(raw).strip():
                            prev_title_count = int(str(raw).strip())
                    except (ValueError, TypeError):
                        prev_title_count = 0
                elif len(previous_data) > 15:
                    # fallback –ø—Ä–µ–∂–Ω–∏–π —Å—Ç–æ–ª–±–µ—Ü 16 (0-–±–∞–∑ 15)
                    try:
                        raw = previous_data[15]
                        if raw and str(raw).strip():
                            prev_title_count = int(str(raw).strip())
                    except (ValueError, TypeError):
                        prev_title_count = 0

                current_title_count = current_headings.get('title_count', 0)
                if prev_title_count != current_title_count:
                    changes['title_count'] = {
                        'difference': current_title_count - prev_title_count,
                        'previous': prev_title_count,
                        'current': current_title_count
                    }

                # Description count
                description_idx = find_idx(['description count', 'description_count'])
                prev_description_count = 0
                if description_idx is not None and len(previous_data) > description_idx:
                    try:
                        raw = previous_data[description_idx]
                        if raw and str(raw).strip():
                            prev_description_count = int(str(raw).strip())
                    except (ValueError, TypeError):
                        prev_description_count = 0
                elif len(previous_data) > 16:
                    # fallback –ø—Ä–µ–∂–Ω–∏–π —Å—Ç–æ–ª–±–µ—Ü 17 (0-–±–∞–∑ 16)
                    try:
                        raw = previous_data[16]
                        if raw and str(raw).strip():
                            prev_description_count = int(str(raw).strip())
                    except (ValueError, TypeError):
                        prev_description_count = 0

                current_description_count = current_headings.get('description_count', 0)
                if prev_description_count != current_description_count:
                    changes['description_count'] = {
                        'difference': current_description_count - prev_description_count,
                        'previous': prev_description_count,
                        'current': current_description_count
                    }

                if changes:
                    return {
                        'status': 'changes_detected',
                        'changes': changes,
                        'errors': []
                    }
                else:
                    return {
                        'status': 'no_changes',
                        'changes': {},
                        'errors': []
                    }
                    
            except (ValueError, IndexError) as e:
                return {
                    'status': 'parsing_error',
                    'changes': {},
                    'errors': [f'–û—à–∏–±–∫–∞ –ø–∞—Ä—Å–∏–Ω–≥–∞ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –¥–∞–Ω–Ω—ã—Ö: {e}']
                }
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è —Å –ø—Ä–µ–¥—ã–¥—É—â–∏–º–∏ –¥–∞–Ω–Ω—ã–º–∏: {e}")
            return {
                'status': 'comparison_error',
                'changes': {},
                'errors': [f'–û—à–∏–±–∫–∞ —Å—Ä–∞–≤–Ω–µ–Ω–∏—è: {e}']
            }


class MultiSiteAnalyzer:
    def __init__(self, sites_config_file: str = 'sites_config.json', delay_between_requests: float = 2.0, max_retries: int = 2, backoff_seconds: float = 0.7, ignore_protection: bool = False, use_cloudscraper: bool = False):
        """
        –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –º—É–ª—å—Ç–∏—Å–∞–π—Ç–æ–≤–æ–≥–æ –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä–∞
        
        Args:
            sites_config_file: –ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–∞–π—Ç–æ–≤
            delay_between_requests: –ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏
            max_retries: –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –ø–æ–≤—Ç–æ—Ä–Ω—ã—Ö –ø–æ–ø—ã—Ç–æ–∫ –ø—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö
            backoff_seconds: –ù–∞—á–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –±—ç–∫–æ—Ñ—Ñ–∞
            ignore_protection: –ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –∑–∞—â–∏—Ç–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–∫–∞–ø—á–∞/Cloudflare)
            use_cloudscraper: –ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å cloudscraper –¥–ª—è –æ–±—Ö–æ–¥–∞ –ø—Ä–æ—Å—Ç—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫
        """
        self.sites_config_file = sites_config_file
        self.config = self.load_sites_config()
        self.sheets_manager = GoogleSheetsServiceAccount()
        
        # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä—É–µ–º Telegram –±–æ—Ç–∞
        self.telegram_bot = TelegramBot()
        if self.telegram_bot.bot_token and self.telegram_bot.chat_id:
            logger.info("Telegram –±–æ—Ç –∏–Ω–∏—Ü–∏–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω")
        else:
            logger.warning("Telegram –±–æ—Ç –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω (–æ—Ç—Å—É—Ç—Å—Ç–≤—É—é—Ç BOT_TOKEN –∏–ª–∏ CHAT_ID)")
        
        # –°–æ–∑–¥–∞–µ–º –ø–∞—Ä—Å–µ—Ä —Å –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–µ–π –∏ sheets_manager
        self.parser = SEOParser(
            delay_between_requests=delay_between_requests,
            config=self.config,
            sheets_manager=self.sheets_manager,
            max_retries=max_retries,
            backoff_seconds=backoff_seconds,
            ignore_protection=ignore_protection,
            use_cloudscraper=use_cloudscraper
        )
        
    def load_sites_config(self) -> Dict:
        """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–∞–π—Ç–æ–≤"""
        if not os.path.exists(self.sites_config_file):
            raise FileNotFoundError(f"–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ {self.sites_config_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
        
        try:
            with open(self.sites_config_file, 'r', encoding='utf-8') as f:
                config = json.load(f)
            logger.info(f"–ö–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—è —Å–∞–π—Ç–æ–≤ –∑–∞–≥—Ä—É–∂–µ–Ω–∞ –∏–∑ {self.sites_config_file}")
            return config
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")
            raise
    
    def get_site_urls(self, site_key: str = None) -> List[str]:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ URL –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        
        Args:
            site_key: –ö–ª—é—á —Å–∞–π—Ç–∞ (–µ—Å–ª–∏ None, –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –≤—Å–µ URL)
            
        Returns:
            –°–ø–∏—Å–æ–∫ URL –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        """
        if site_key:
            if site_key not in self.config['sites']:
                # –î–ª—è —Ç–µ—Å—Ç–æ–≤ –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –ø—É—Å—Ç–æ–π —Å–ø–∏—Å–æ–∫, –∞ –Ω–µ –∏—Å–∫–ª—é—á–µ–Ω–∏–µ
                return []
            return self.config['sites'][site_key]['urls']
        else:
            # –í–æ–∑–≤—Ä–∞—â–∞–µ–º –≤—Å–µ URL –≤—Å–µ—Ö —Å–∞–π—Ç–æ–≤
            all_urls = []
            for site_key, site_data in self.config['sites'].items():
                all_urls.extend(site_data['urls'])
            return all_urls
    
    def get_site_info(self, url: str) -> Dict:
        """
        –ü–æ–ª—É—á–µ–Ω–∏–µ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∞–π—Ç–µ –ø–æ URL
        
        Args:
            url: URL —Å—Ç—Ä–∞–Ω–∏—Ü—ã
            
        Returns:
            –ò–Ω—Ñ–æ—Ä–º–∞—Ü–∏—è –æ —Å–∞–π—Ç–µ
        """
        for site_key, site_data in self.config['sites'].items():
            if any(url.startswith(site_url) for site_url in site_data['urls']):
                return {
                    'key': site_key,
                    'name': site_data['name'],
                    'base_url': site_data['base_url'],
                    'description': site_data['description']
                }
        return {'key': 'unknown', 'name': '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω—ã–π —Å–∞–π—Ç', 'base_url': '', 'description': ''}
    
    def run_analysis(self, site_key: str = None, custom_urls: List[str] = None) -> Dict[str, List[Dict]]:
        """
        –ó–∞–ø—É—Å–∫ –∞–Ω–∞–ª–∏–∑–∞ –¥–ª—è —Å–∞–π—Ç–æ–≤
        
        Args:
            site_key: –ö–ª—é—á –∫–æ–Ω–∫—Ä–µ—Ç–Ω–æ–≥–æ —Å–∞–π—Ç–∞ (–µ—Å–ª–∏ None, –∞–Ω–∞–ª–∏–∑–∏—Ä—É–µ—Ç –≤—Å–µ)
            custom_urls: –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ URL (–ø–µ—Ä–µ–æ–ø—Ä–µ–¥–µ–ª—è–µ—Ç –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏—é)
            
        Returns:
            –°–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –ø–æ —Å–∞–π—Ç–∞–º
        """
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º URL –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞
        if custom_urls:
            urls_to_analyze = custom_urls
            logger.info(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ URL: {len(custom_urls)}")
        elif site_key:
            urls_to_analyze = self.get_site_urls(site_key)
            logger.info(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å–∞–π—Ç '{site_key}': {len(urls_to_analyze)} URL")
        else:
            urls_to_analyze = self.get_site_urls()
            logger.info(f"–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –≤—Å–µ —Å–∞–π—Ç—ã: {len(urls_to_analyze)} URL")
        
        # –ì—Ä—É–ø–ø–∏—Ä—É–µ–º URL –ø–æ —Å–∞–π—Ç–∞–º
        sites_results = {}
        
        for url in urls_to_analyze:
            site_info = self.get_site_info(url)
            site_key = site_info['key']
            
            if site_key not in sites_results:
                sites_results[site_key] = {
                    'site_info': site_info,
                    'results': []
                }
            
            # –ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º —Å—Ç—Ä–∞–Ω–∏—Ü—É
            try:
                result = self.parser.analyze_page(url)
                sites_results[site_key]['results'].append(result)
                logger.info(f"–ü—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–∞ —Å—Ç—Ä–∞–Ω–∏—Ü–∞: {url}")
            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –∞–Ω–∞–ª–∏–∑–∞ {url}: {e}")
                sites_results[site_key]['results'].append({
                    'url': url,
                    'timestamp': datetime.now().isoformat(),
                    'status': 'error',
                    'error': str(e)
                })
        
        return sites_results
    
    def print_results(self, sites_results: Dict[str, List[Dict]]):
        """–í—ã–≤–æ–¥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –∞–Ω–∞–ª–∏–∑–∞"""
        print("\n" + "="*100)
        print("–†–ï–ó–£–õ–¨–¢–ê–¢–´ –ú–£–õ–¨–¢–ò–°–ê–ô–¢–û–í–û–ì–û SEO –ê–ù–ê–õ–ò–ó–ê")
        print("="*100)
        
        total_sites = len(sites_results)
        total_pages = sum(len(site_data['results']) for site_data in sites_results.values())
        successful_pages = 0
        
        for site_key, site_data in sites_results.items():
            site_info = site_data['site_info']
            results = site_data['results']
            
            print(f"\nüåê –°–ê–ô–¢: {site_info['name']} ({site_key})")
            print(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {site_info['description']}")
            print(f"   –ë–∞–∑–æ–≤—ã–π URL: {site_info['base_url']}")
            print(f"   –°—Ç—Ä–∞–Ω–∏—Ü –ø—Ä–æ–∞–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞–Ω–æ: {len(results)}")
            print("-" * 80)
            
            site_successful = 0
            
            for i, result in enumerate(results, 1):
                print(f"\n   {i}. {result['url']}")
                print(f"      –°—Ç–∞—Ç—É—Å: {result['status']}")
                
                if result['status'] == 'success':
                    site_successful += 1
                    successful_pages += 1
                    
                    headings = result['headings']
                    print("      üìà –ö–æ–ª–∏—á–µ—Å—Ç–≤–æ –∑–∞–≥–æ–ª–æ–≤–∫–æ–≤:")
                    
                    for j in range(1, 7):
                        tag = f'h{j}'
                        total = headings.get(f'{tag}_total', 0)
                        non_empty = headings.get(f'{tag}_non_empty', 0)
                        
                        if total > 0:
                            print(f"        {tag.upper()}: {non_empty} (–≤—Å–µ–≥–æ: {total})")

                    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏—é –æ Title –∏ Description
                    title_count = headings.get('title_count', 0)
                    title_result = headings.get('title_result', '')
                    description_count = headings.get('description_count', 0)
                    description_result = headings.get('description_result', '')

                    print("      üìù Title –∏ Description:")
                    print(f"        Title: {title_result}")
                    print(f"        Description: {description_result}")
                    
                    # –ü—Ä–æ–≤–µ—Ä—è–µ–º —Å—Ä–∞–≤–Ω–µ–Ω–∏–µ
                    if 'comparison' in result:
                        comparison = result['comparison']
                        print(f"      üîç –°—Ä–∞–≤–Ω–µ–Ω–∏–µ: {comparison['status']}")
                        
                        if comparison.get('errors'):
                            print("      ‚ùå –û—à–∏–±–∫–∏:")
                            for error in comparison['errors']:
                                print(f"        - {error}")
                        
                        if comparison.get('changes'):
                            print("      ‚úÖ –ò–∑–º–µ–Ω–µ–Ω–∏—è:")
                            for change_type, change_data in comparison['changes'].items():
                                print(f"        - {change_type}: +{change_data['difference']}")
                else:
                    print(f"      ‚ùå –û—à–∏–±–∫–∞: {result.get('error', '–ù–µ–∏–∑–≤–µ—Å—Ç–Ω–∞—è –æ—à–∏–±–∫–∞')}")
            
            print(f"\n   üìä –°—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ —Å–∞–π—Ç–∞: {site_successful}/{len(results)} —É—Å–ø–µ—à–Ω–æ")
        
        print("\n" + "="*100)
        print(f"üìà –û–ë–©–ê–Ø –°–¢–ê–¢–ò–°–¢–ò–ö–ê:")
        print(f"   üåê –°–∞–π—Ç–æ–≤: {total_sites}")
        print(f"   üìÑ –°—Ç—Ä–∞–Ω–∏—Ü: {total_pages}")
        print(f"   ‚úÖ –£—Å–ø–µ—à–Ω–æ: {successful_pages}")
        print(f"   ‚ùå –û—à–∏–±–æ–∫: {total_pages - successful_pages}")
        print(f"   üìä –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: {(successful_pages/total_pages*100):.1f}%" if total_pages > 0 else "   üìä –ü—Ä–æ—Ü–µ–Ω—Ç —É—Å–ø–µ—Ö–∞: 0%")
        print("="*100)
    
    def save_results_locally(self, sites_results: Dict[str, List[Dict]]):
        """–°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ –ª–æ–∫–∞–ª—å–Ω—ã–π —Ñ–∞–π–ª"""
        try:
            timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
            filename = f'multi_site_results_{timestamp}.json'
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(sites_results, f, ensure_ascii=False, indent=2)
            
            logger.info(f"–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
            print(f"üíæ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ {filename}")
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}")
    
    def upload_to_sheets(self, sites_results: Dict[str, List[Dict]]):
        """–ó–∞–≥—Ä—É–∑–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Google Sheets"""
        try:
            settings = self.config['default_settings']
            spreadsheet_id = settings.get('spreadsheet_id')
            sheet_name = settings.get('sheet_name', '–õ–∏—Å—Ç1')
            service_account_file = settings.get('service_account_file', 'service-account-key.json')
            
            if not spreadsheet_id:
                logger.error("–ù–µ —É–∫–∞–∑–∞–Ω ID —Ç–∞–±–ª–∏—Ü—ã –≤ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏")
                return
            
            # –û–±–Ω–æ–≤–ª—è–µ–º —Ñ–∞–π–ª Service Account –≤ –º–µ–Ω–µ–¥–∂–µ—Ä–µ
            self.sheets_manager.service_account_file = service_account_file
            
            # –ü–æ–¥–≥–æ—Ç–∞–≤–ª–∏–≤–∞–µ–º –≤—Å–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –¥–ª—è –∑–∞–≥—Ä—É–∑–∫–∏ (–±–µ–∑ –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ —Å–∞–π—Ç–∞—Ö)
            all_results = []
            for site_key, site_data in sites_results.items():
                all_results.extend(site_data['results'])
            
            logger.info(f"–ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Google Sheets: {spreadsheet_id}")
            
            success = self.sheets_manager.upload_results(
                spreadsheet_id, all_results, sheet_name
            )
            
            if success:
                logger.info("–†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ Google Sheets")
                print("‚úÖ –†–µ–∑—É–ª—å—Ç–∞—Ç—ã —É—Å–ø–µ—à–Ω–æ –∑–∞–≥—Ä—É–∂–µ–Ω—ã –≤ Google Sheets")
            else:
                logger.error("–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Google Sheets")
                print("‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤ –≤ Google Sheets")
                
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Google Sheets: {e}")
            print(f"‚ùå –û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –≤ Google Sheets: {e}")
    
    def send_telegram_report(self, sites_results: Dict[str, List[Dict]]):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ –æ—Ç—á–µ—Ç–∞ –≤ Telegram"""
        try:
            if not self.telegram_bot.bot_token or not self.telegram_bot.chat_id:
                logger.warning("Telegram –±–æ—Ç –Ω–µ –Ω–∞—Å—Ç—Ä–æ–µ–Ω, –ø—Ä–æ–ø—É—Å–∫–∞–µ–º –æ—Ç–ø—Ä–∞–≤–∫—É –æ—Ç—á–µ—Ç–∞")
                return

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Å–Ω–æ–≤–Ω—É—é —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É
            success = self.telegram_bot.send_statistics(sites_results)
            if success:
                logger.info("–û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –≤ Telegram")
                print("üì± –û—Å–Ω–æ–≤–Ω–∞—è —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∞ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω–∞ –≤ Telegram")
            else:
                logger.error("–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤ Telegram")
                print("‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –æ—Å–Ω–æ–≤–Ω–æ–π —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫–∏ –≤ Telegram")

            # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –¥–µ—Ç–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è, –µ—Å–ª–∏ –æ–Ω–∏ –µ—Å—Ç—å
            changes_success = self.telegram_bot.send_detailed_changes(sites_results)
            if changes_success:
                logger.info("–î–µ—Ç–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –≤ Telegram")
                print("üì± –î–µ—Ç–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã –≤ Telegram")
            else:
                logger.info("–î–µ—Ç–∞–ª—å–Ω—ã–µ –∏–∑–º–µ–Ω–µ–Ω–∏—è –Ω–µ –æ—Ç–ø—Ä–∞–≤–ª–µ–Ω—ã (–Ω–µ—Ç –∏–∑–º–µ–Ω–µ–Ω–∏–π –∏–ª–∏ –æ—à–∏–±–∫–∞)")

        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram: {e}")
            print(f"‚ùå –û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ –≤ Telegram: {e}")
    
    def send_telegram_error(self, error_message: str):
        """–û—Ç–ø—Ä–∞–≤–∫–∞ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–µ –≤ Telegram"""
        try:
            if not self.telegram_bot.bot_token or not self.telegram_bot.chat_id:
                return
            
            self.telegram_bot.send_error_notification(error_message)
            
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ—Ç–ø—Ä–∞–≤–∫–∏ —É–≤–µ–¥–æ–º–ª–µ–Ω–∏—è –æ–± –æ—à–∏–±–∫–µ –≤ Telegram: {e}")
    
    def list_available_sites(self):
        """–í—ã–≤–æ–¥ —Å–ø–∏—Å–∫–∞ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–∞–π—Ç–æ–≤"""
        print("\nüìã –î–û–°–¢–£–ü–ù–´–ï –°–ê–ô–¢–´:")
        print("="*60)
        
        for site_key, site_data in self.config['sites'].items():
            print(f"\nüåê {site_data['name']} ({site_key})")
            print(f"   –û–ø–∏—Å–∞–Ω–∏–µ: {site_data['description']}")
            print(f"   –ë–∞–∑–æ–≤—ã–π URL: {site_data['base_url']}")
            print(f"   –°—Ç—Ä–∞–Ω–∏—Ü –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞: {len(site_data['urls'])}")
            print("   URL:")
            for url in site_data['urls']:
                print(f"     - {url}")

    def analyze_url(self, url: str) -> dict:
        """–ê–Ω–∞–ª–∏–∑ –æ–¥–Ω–æ–π —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–¥–ª—è —Ç–µ—Å—Ç–æ–≤)"""
        result = self.parser.analyze_page(url)
        # –ü—Ä–∏–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç –∫ —Ñ–æ—Ä–º–∞—Ç—É, –æ–∂–∏–¥–∞–µ–º–æ–º—É —Ç–µ—Å—Ç–∞–º–∏
        headings = result.get('headings', {})
        return {
            'url': result.get('url'),
            'status_code': result.get('status_code'),
            'h1_count': headings.get('h1_non_empty', 0),
            'h2_count': headings.get('h2_non_empty', 0),
            'h3_count': headings.get('h3_non_empty', 0),
            'total_headings': headings.get('total_headings', 0),
            'title_count': headings.get('title_count', 0),
            'title_result': headings.get('title_result', ''),
            'description_count': headings.get('description_count', 0),
            'description_result': headings.get('description_result', ''),
            'error': result.get('error'),
        }

    def analyze_site(self, site_key: str) -> list:
        """–ê–Ω–∞–ª–∏–∑ –≤—Å–µ—Ö —Å—Ç—Ä–∞–Ω–∏—Ü —Å–∞–π—Ç–∞ (–¥–ª—è —Ç–µ—Å—Ç–æ–≤)"""
        urls = self.get_site_urls(site_key)
        results = []
        for url in urls:
            results.append(self.analyze_url(url))
        return results

    def save_results(self, results: list, filename: str):
        """–°–æ—Ö—Ä–∞–Ω—è–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –∞–Ω–∞–ª–∏–∑–∞ –≤ —Ñ–∞–π–ª (–¥–ª—è —Ç–µ—Å—Ç–æ–≤)"""
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(results, f, ensure_ascii=False, indent=2)


def load_config(config_file: str) -> Dict[str, Any]:
    """–ó–∞–≥—Ä—É–∑–∫–∞ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ –∏–∑ —Ñ–∞–π–ª–∞"""
    if not os.path.exists(config_file):
        raise FileNotFoundError(f"–§–∞–π–ª –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ {config_file} –Ω–µ –Ω–∞–π–¥–µ–Ω")
    
    try:
        with open(config_file, 'r', encoding='utf-8') as f:
            config = json.load(f)
        return config
    except Exception as e:
        raise Exception(f"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏: {e}")


def main():
    """–û—Å–Ω–æ–≤–Ω–∞—è —Ñ—É–Ω–∫—Ü–∏—è"""
    parser = argparse.ArgumentParser(description='–ú—É–ª—å—Ç–∏—Å–∞–π—Ç–æ–≤—ã–π SEO –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä')
    parser.add_argument('--config', '-c', default='sites_config.json', 
                       help='–ü—É—Ç—å –∫ —Ñ–∞–π–ª—É –∫–æ–Ω—Ñ–∏–≥—É—Ä–∞—Ü–∏–∏ —Å–∞–π—Ç–æ–≤')
    parser.add_argument('--site', '-s', 
                       help='–ê–Ω–∞–ª–∏–∑–∏—Ä–æ–≤–∞—Ç—å –∫–æ–Ω–∫—Ä–µ—Ç–Ω—ã–π —Å–∞–π—Ç (piter-online, moskva-online, 101internet)')
    parser.add_argument('--list-sites', action='store_true',
                       help='–ü–æ–∫–∞–∑–∞—Ç—å —Å–ø–∏—Å–æ–∫ –¥–æ—Å—Ç—É–ø–Ω—ã—Ö —Å–∞–π—Ç–æ–≤')
    parser.add_argument('--urls', nargs='+',
                       help='–ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–µ URL –¥–ª—è –∞–Ω–∞–ª–∏–∑–∞')
    parser.add_argument('--no-sheets', action='store_true',
                       help='–ù–µ –∑–∞–≥—Ä—É–∂–∞—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Google Sheets')
    parser.add_argument('--no-local', action='store_true',
                       help='–ù–µ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ª–æ–∫–∞–ª—å–Ω–æ')
    parser.add_argument('--no-telegram', action='store_true',
                       help='–ù–µ –æ—Ç–ø—Ä–∞–≤–ª—è—Ç—å –æ—Ç—á–µ—Ç –≤ Telegram')
    parser.add_argument('--no-log', action='store_true',
                       help='–ù–µ –∑–∞–ø–∏—Å—ã–≤–∞—Ç—å –ª–æ–≥–∏ –≤ —Ñ–∞–π–ª')
    parser.add_argument('--delay', type=float, default=2.0,
                       help='–ó–∞–¥–µ—Ä–∂–∫–∞ –º–µ–∂–¥—É –∑–∞–ø—Ä–æ—Å–∞–º–∏, —Å–µ–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2.0)')
    parser.add_argument('--max-retries', type=int, default=2,
                       help='–ß–∏—Å–ª–æ –ø–æ–≤—Ç–æ—Ä–æ–≤ –ø—Ä–∏ –≤—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ—à–∏–±–∫–∞—Ö (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 2)')
    parser.add_argument('--backoff', type=float, default=0.7,
                       help='–ù–∞—á–∞–ª—å–Ω–∞—è –∑–∞–¥–µ—Ä–∂–∫–∞ –¥–ª—è —ç–∫—Å–ø–æ–Ω–µ–Ω—Ü–∏–∞–ª—å–Ω–æ–≥–æ –±—ç–∫–æ—Ñ—Ñ–∞, —Å–µ–∫ (–ø–æ —É–º–æ–ª—á–∞–Ω–∏—é 0.7)')
    parser.add_argument('--ignore-protection', action='store_true',
                       help='–ò–≥–Ω–æ—Ä–∏—Ä–æ–≤–∞—Ç—å –∑–∞—â–∏—Ç–Ω—ã–µ —Å—Ç—Ä–∞–Ω–∏—Ü—ã (–∫–∞–ø—á–∞/Cloudflare)')
    parser.add_argument('--use-cloudscraper', action='store_true',
                       help='–ò—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å cloudscraper –¥–ª—è –æ–±—Ö–æ–¥–∞ –ø—Ä–æ—Å—Ç—ã—Ö –ø—Ä–æ–≤–µ—Ä–æ–∫')
    
    args = parser.parse_args()
    
    # –ù–∞—Å—Ç—Ä–æ–π–∫–∞ –ª–æ–≥–∏—Ä–æ–≤–∞–Ω–∏—è
    if args.no_log:
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[logging.StreamHandler()]
        )
    else:
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('multi_site_analyzer.log', encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
    
    logger = logging.getLogger(__name__)
    
    try:
        # –°–æ–∑–¥–∞–µ–º –∞–Ω–∞–ª–∏–∑–∞—Ç–æ—Ä
        analyzer = MultiSiteAnalyzer(
            sites_config_file=args.config,
            delay_between_requests=args.delay,
            max_retries=args.max_retries,
            backoff_seconds=args.backoff,
            ignore_protection=args.ignore_protection,
            use_cloudscraper=args.use_cloudscraper
        )
        
        # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —Å–ø–∏—Å–æ–∫ —Å–∞–π—Ç–æ–≤
        if args.list_sites:
            analyzer.list_available_sites()
            return
        
        # –ó–∞–ø—É—Å–∫–∞–µ–º –∞–Ω–∞–ª–∏–∑
        results = analyzer.run_analysis(
            site_key=args.site,
            custom_urls=args.urls
        )
        
        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        analyzer.print_results(results)
        
        # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ª–æ–∫–∞–ª—å–Ω–æ
        if not args.no_local:
            analyzer.save_results_locally(results)
        
        # –ó–∞–≥—Ä—É–∂–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ Google Sheets
        if not args.no_sheets:
            analyzer.upload_to_sheets(results)
        
        # –û—Ç–ø—Ä–∞–≤–ª—è–µ–º –æ—Ç—á–µ—Ç –≤ Telegram
        if not args.no_telegram:
            analyzer.send_telegram_report(results)
        
        print(f"\nüéâ –ê–Ω–∞–ª–∏–∑ –∑–∞–≤–µ—Ä—à–µ–Ω —É—Å–ø–µ—à–Ω–æ!")
            
    except KeyboardInterrupt:
        print("\n‚èπÔ∏è –ê–Ω–∞–ª–∏–∑ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        try:
            if 'analyzer' in locals() and getattr(analyzer, 'telegram_bot', None):
                analyzer.send_telegram_error("–ê–Ω–∞–ª–∏–∑ –ø—Ä–µ—Ä–≤–∞–Ω –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–º")
        except Exception:
            pass
    except Exception as e:
        logger.error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        print(f"‚ùå –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        try:
            if 'analyzer' in locals() and getattr(analyzer, 'telegram_bot', None):
                analyzer.send_telegram_error(f"–ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞: {e}")
        except Exception:
            pass
        sys.exit(1)


if __name__ == "__main__":
    main()
